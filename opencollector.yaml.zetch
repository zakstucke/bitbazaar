receivers:
  # Where programs/open telemetry sdks should send their data:
  otlp:
    protocols:
      grpc:
        endpoint: localhost:4317
      http:
        endpoint: localhost:4318
        cors:
          allowed_origins:
            - "*"

processors:
  # Prevent memory usage from exceeding 40% of the total available RAM,
  # will start applying backpressure to the sources when memory usage exceeds 25% of the total available RAM
  memory_limiter:
    check_interval: 1s
    limit_percentage: 40
    spike_limit_percentage: 25
  batch:
    # If hit this number before timeout, will send batch:
    send_batch_size: 10000
    # Will always send after this timeout, even if send_batch_size not reached yet:
    timeout: {% if DEBUG %}1s{% else %}30s{% endif %}

exporters:
  otlphttp:
    endpoint: {{ OTLP_OO_ENDPOINT }}
    compression: gzip
    headers:
      Authorization: {{ OTLP_OO_AUTH }}
      stream-name: default

{%- if DEBUG %}
  # Writes all opentelemetry logs, traces, metrics to a file, useful for testing:
  file/debug_file_writing:
    path: {{ ROOT_DIR }}/logs/otlp_telemetry_out.log
    rotation:
      max_megabytes: 10
      max_days: 3
      max_backups: 3
      localtime: true
    flush_interval: 1  # Write every 1 seconds
{%- endif %}

service:
  telemetry:
    logs:
      level: "info"

  pipelines:
    logs:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters:
        - otlphttp
        {%- if DEBUG %}
        - file/debug_file_writing
        {%- endif %}
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters:
        - otlphttp
        {%- if DEBUG %}
        - file/debug_file_writing
        {%- endif %}
    metrics:  # Don't bother with memory limiting for metrics, traces and logs will be the largest consumers of memory
      receivers: [otlp]
      processors: [batch]
      exporters:
        - otlphttp
        {%- if DEBUG %}
        - file/debug_file_writing
        {%- endif %}